# SLU paper



[中文](README.md) | English





## 1 Introduction

This project is used to carry out the code implementation and reading of papers presented by Spoken Language Understanding (SLU).

The papers to be read are:

SLU paper

[Uncorrelated Joint Modeling]

01. "Attention-Based Recurrent Neural Network Models for Joint Intent Detection and Slot Filling"

02. BERT for Joint Intent Classification and Slot Filling

[Joint modeling of intention Association slots]

03. A Stack-Propagation Framework with Token-Level Intent Detection for Spoken Language Understanding (Spoken Language Understanding)

04. Slot-gated Modeling for Joint Slot Filling and Intent Prediction

05. A Self-Attention Joint Model for Spoken Language Understanding in Situational Dialog Applications (Spoken Language Understanding in Situational Dialog)

06. "A Self-Attentive Model with Gate Mechanism for Spoken Language Understanding"

[Bidirectional Association Joint Modeling]

07. A Novel Bi-directional Model for Joint Intent Detection and Slot Filling

08. Joint Slot Filling and Intent Detection via Capsule Neural Networks

309. JOINT INTENT DETECTION AND SLOT FILLING BASED ON CONTINUAL LEARNING MODEL

10. Focus on Interaction: A Novel Dynamic Graph Model for Joint Multiple Intent Detection and Slot Filling

11. Encoding Syntactic Knowledge in Transformer Encoder for Intent Detection and Slot Filling

12. A Two-Stage Selective Fusion Framework for Joint Intent Detection and Slot Filling

13. Joint intent detection and slot filling with wheel-graph attention networks

14. A Graph Attention Interactive Refine Framework with Contextual Regularization for Jointing Intent Detection and Slot Filling."

15. A joint model based on interactive gate mechanism for spoken language understanding (Spoken Language Understanding)

16. Learning High-Order Semantic Representation for Intent Classification and Slot Filling on Low-Resource Language via Hypergraph."





## 2 resolved paper Directory (under update)



The list of papers with existing code is:



- 01 Attention-Based Recurrent Neural Network Models for Joint Intent Detection and Slot Filling
    Learning website:https://www.jianshu.com/p/cec045c55175
    GitHub：https://github.com/DSKSD/RNN-for-Joint-NLU
- 02 BERT for Joint Intent Classification and Slot Filling
    Learning website:https://www.jianshu.com/p/2144cb5b222f
    GitHub：https://github.com/monologg/JointBERT
- 03 A Stack-Propagation Framework with Token-Level Intent Detection for Spoken Language Understanding
    Learning website:https://www.jianshu.com/p/70b6d594bfab
    GitHub：https://github.com/LeePleased/StackPropagation-SLU
- 04 Slot-Gated Modeling for Joint Slot Filling and Intent Prediction
    Learning website:http://events.jianshu.io/p/8b3c6a9685e4
    GitHub：https://github.com/MiuLab/SlotGated-SLU
- 06 A Self-Attentive Model with Gate Mechanism for Spoken Language Understanding
    Learning website:https://www.jianshu.com/p/5cfbf1ddea74
    GitHub： https://github.com/NinedayWang/Self-Attentive-and-Gated-SLU
- 07 A Novel Bi-directional Interrelated Model for Joint Intent Detection and Slot Filling
    Learning website:https://www.jianshu.com/p/6c76f31f9e20
    GitHub：https://github.com/ZephyrChenzf/SF-ID-Network-For-NLU
- 08 Joint Slot Filling and Intent Detection via Capsule Neural Networks
    Learning website:https://blog.csdn.net/u012852385/article/details/104346304
    GitHub：https://github.com/czhang99/Capsule-NLU
- 14 A Graph Attention Interactive Refine Framework with Contextual Regularization for Jointing Intent Detection and Slot Filling
    GitHub：https://github.com/BillKiller/GAIR-SLU

## 3 Thank



Thank you for reading!

I wish you a full table of papers!